# ğŸ§ª Taller - Interfaces Multimodales: Uniendo Voz y Gestos

## ğŸ“… Fecha
`2025-05-31`

---

## ğŸ¯ Objetivo del Taller
Fusionar gestos (detectados con MediaPipe) y comandos de voz para realizar acciones compuestas dentro de una interfaz visual. Este taller introduce los fundamentos de los sistemas de interacciÃ³n multimodal, combinando dos formas de entrada humana para enriquecer la experiencia de control.

---

## ğŸ§  Conceptos Aprendidos

- [X] InteracciÃ³n multimodal: combinaciÃ³n de voz y gestos.
- [X] DetecciÃ³n de gestos con MediaPipe.
- [X] Reconocimiento de voz con SpeechRecognition y PyAudio.
- [X] ComunicaciÃ³n entre Python y Processing usando OSC.
- [X] SincronizaciÃ³n de entradas de voz y gestos para acciones compuestas.

---

## ğŸ”§ Herramientas y Entornos


- Python (`opencv-python`, `mediapipe`, `speech_recognition`, `pyaudio`, `tkinter`, `threading`)
- Processing (`oscP5`)

---

## ğŸ“ Estructura del Proyecto

```
2025-05-31_taller_interfaces_multimodales_voz_gestos/
â”œâ”€â”€ processing/            # Processing
â”œâ”€â”€ python/                # Python
â”œâ”€â”€ resultados/            # capturas, mÃ©tricas, gifs
â”œâ”€â”€ README.md
```

---

## ğŸ§ª ImplementaciÃ³n


### ğŸ”¹ Etapas realizadas
1. ConfiguraciÃ³n del entorno de Python con las librerÃ­as necesarias.
2. ImplementaciÃ³n de la captura de video y detecciÃ³n de gestos con MediaPipe.
3. ImplementaciÃ³n del reconocimiento de voz con SpeechRecognition y PyAudio.
4. ConfiguraciÃ³n de la comunicaciÃ³n OSC entre Python y Processing.
5. ImplementaciÃ³n de la lÃ³gica para combinar gestos y comandos de voz.
6. CreaciÃ³n de una interfaz visual en Processing que responda a los comandos recibidos.
7. Pruebas y ajustes para asegurar una interacciÃ³n fluida y en tiempo real.


### ğŸ”¹ CÃ³digo relevante


#### Python

```python
def escuchar_voz(app):
    r = sr.Recognizer()
    with sr.Microphone() as source:
        while True:
            try:
                audio = r.listen(source, timeout=5, phrase_time_limit=3)
                texto = r.recognize_google(audio, language="es-CO").lower()
```

```python
def detectar_gesto_mano(hand_landmarks, mp_hands):
    dedos = {
        "PULGAR": (mp_hands.HandLandmark.THUMB_TIP, mp_hands.HandLandmark.THUMB_IP),
        "INDICE": (mp_hands.HandLandmark.INDEX_FINGER_TIP, mp_hands.HandLandmark.INDEX_FINGER_PIP),
        "MEDIO": (mp_hands.HandLandmark.MIDDLE_FINGER_TIP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP),
        "ANULAR": (mp_hands.HandLandmark.RING_FINGER_TIP, mp_hands.HandLandmark.RING_FINGER_PIP),
        "MENIQUE": (mp_hands.HandLandmark.PINKY_TIP, mp_hands.HandLandmark.PINKY_PIP),
    }
    extendidos = {}
    for nombre, (tip, pip) in dedos.items():
        extendidos[nombre] = hand_landmarks.landmark[tip].y < hand_landmarks.landmark[pip].y


    if all(extendidos.values()):
        return "mano_abierta"
    ...
```
        

```python
def detectar_gestos():
    mp_hands = mp.solutions.hands
    mp_dibujo = mp.solutions.drawing_utils
    hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)
    cap = cv2.VideoCapture(0)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands.process(rgb_frame)

        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                mp_dibujo.draw_landmarks(
                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS
                )
            
                estado_gesto = detectar_gesto_mano(hand_landmarks, mp_hands)
```

#### Processing

```java
osc = new OscP5(this, 9124);
```

```java
void oscEvent(OscMessage theOscMessage) {
  if (theOscMessage.checkAddrPattern("/accion")) {
    String accion = theOscMessage.get(0).stringValue();
    println("Comando recibido: " + accion);
    ultimoComando = accion;

    if (accion.equals("iniciar_animacion")) {
      animar = true;
    } else if (accion.equals("cambiar_color")) {
      fondoColor = color(random(255), random(255), random(255));
    } else if (accion.equals("mover_objeto")) {
      shapeX = random(width);
      shapeY = random(height);
    } else if (accion.equals("cambiar_forma")) {
      forma = (forma + 1) % 3; // Cambiar entre formas
    } else if (accion.equals("detener_animacion")) {
      animar = false;
    }
  }
}
```



---
## ğŸ“Š Resultados Visuales


### Python + Processing
![Test](resultados/test.gif)


---

## ğŸ§© Prompts Usados

### Processing
```text
En Processing, crea una escena visual interactiva que reciba comandos desde una aplicaciÃ³n en Python a travÃ©s de mensajes OSC. Utiliza la librerÃ­a oscP5 para escuchar los mensajes entrantes y vincula cada tipo de mensaje a una acciÃ³n especÃ­fica: cambiar el color o forma de objetos en pantalla, iniciar o detener animaciones, y mostrar texto dinÃ¡mico que indique el Ãºltimo comando recibido. Asegura una comunicaciÃ³n fluida y en tiempo real entre ambos entornos para una respuesta visual inmediata a las entradas del usuario.
```

### Python
```text
En un entorno Python (Colab o local), utiliza mediapipe y opencv-python para capturar video en tiempo real desde una webcam y detectar gestos de mano, mientras empleas speech_recognition y pyaudio para reconocer comandos de voz simples como 'azul', 'rotar' o 'mostrar'. Procesa simultÃ¡neamente ambas seÃ±ales de entrada utilizando hilos o bucles coordinados, e implementa lÃ³gica condicional que combine ambos tipos de entrada. Por ejemplo: cambiar el color solo si se dice 'cambiar' y la mano estÃ¡ abierta, o mover un objeto solo si se pronuncia 'mover' y se muestra un gesto de dos dedos. Finalmente, diseÃ±a una escena visual interactiva con tkinter, que responda dinÃ¡micamente a estas combinaciones de entrada.
```



---

## ğŸ’¬ ReflexiÃ³n Final

- Â¿QuÃ© aprendiste o reforzaste con este taller?
AprendÃ­ a integrar gestos con manos con MediaPipe y openCV, y a combinarlo con comandos de voz usando SpeechRecognition. La comunicaciÃ³n entre Python y Processing a travÃ©s de OSC fue un desafÃ­o interesante que mejorÃ³ mi comprensiÃ³n de la interacciÃ³n multimodal.
- Â¿QuÃ© parte fue mÃ¡s compleja o interesante?
La parte mÃ¡s compleja fue la sincronizaciÃ³n de las entradas de voz y gestos, especialmente al manejar mÃºltiples condiciones para activar acciones especÃ­ficas. 
- Â¿QuÃ© mejorarÃ­as o quÃ© aplicarÃ­as en futuros proyectos?
MejorarÃ­a la robustez del sistema para manejar errores de reconocimiento de voz y gestos, y explorarÃ­a mÃ¡s formas de interacciÃ³n multimodal. TambiÃ©n considerarÃ­a optimizar la latencia en la comunicaciÃ³n entre Python y Processing para una experiencia mÃ¡s fluida.

---

## âœ… Checklist de Entrega

- [X] Carpeta `2025-05-31_taller_interfaces_multimodales_voz_gestos`
- [X] CÃ³digo limpio y funcional
- [X] GIF incluido con nombre descriptivo
- [X] README completo y claro
- [X] Commits descriptivos en inglÃ©s

---