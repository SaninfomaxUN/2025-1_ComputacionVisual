# ğŸ§ª Taller - Interfaces Multimodales: Uniendo Voz y Gestos

## ğŸ“… Fecha
`2025-05-31`

---

## ğŸ¯ Objetivo del Taller
Fusionar gestos (detectados con MediaPipe) y comandos de voz para realizar acciones compuestas dentro de una interfaz visual. Este taller introduce los fundamentos de los sistemas de interacciÃ³n multimodal, combinando dos formas de entrada humana para enriquecer la experiencia de control.

---

## ğŸ§  Conceptos Aprendidos

- [ ] Transformaciones geomÃ©tricas (escala, rotaciÃ³n, traslaciÃ³n)
- [ ] SegmentaciÃ³n de imÃ¡genes
- [ ] Shaders y efectos visuales
- [ ] Entrenamiento de modelos IA
- [ ] ComunicaciÃ³n por gestos o voz
- [ ] Otro: _______________________

---

## ğŸ”§ Herramientas y Entornos


- Python (`opencv-python`, `torch`, `mediapipe`, `diffusers`, etc.)
- Processing

---

## ğŸ“ Estructura del Proyecto

```
2025-05-31_taller_interfaces_multimodales_voz_gestos/
â”œâ”€â”€ processing/            # Processing
â”œâ”€â”€ python/                # Python
â”œâ”€â”€ datos/                 # imÃ¡genes, audio, modelos, video
â”œâ”€â”€ resultados/            # capturas, mÃ©tricas, gifs
â”œâ”€â”€ README.md
```

---

## ğŸ§ª ImplementaciÃ³n


### ğŸ”¹ Etapas realizadas
1. PreparaciÃ³n de datos o escena.
2. AplicaciÃ³n de modelo o algoritmo.
3. VisualizaciÃ³n o interacciÃ³n.
4. Guardado de resultados.


### ğŸ”¹ CÃ³digo relevante


#### Python

```python
# example
Code snippet
```

#### Processing

```java
// example
Code snippet
```



---
## ğŸ“Š Resultados Visuales


### Processing
![Processing](resultados/Processing.gif)

### Python
![Python](resultados/Python.gif)


---

## ğŸ§© Prompts Usados

### Processing
```text
En Processing, crea una escena visual interactiva que reciba comandos desde una aplicaciÃ³n en Python a travÃ©s de mensajes OSC. Utiliza la librerÃ­a oscP5 para escuchar los mensajes entrantes y vincula cada tipo de mensaje a una acciÃ³n especÃ­fica: cambiar el color o forma de objetos en pantalla, iniciar o detener animaciones, y mostrar texto dinÃ¡mico que indique el Ãºltimo comando recibido. Asegura una comunicaciÃ³n fluida y en tiempo real entre ambos entornos para una respuesta visual inmediata a las entradas del usuario.
```

### Python
```text
En un entorno Python (Colab o local), utiliza mediapipe y opencv-python para capturar video en tiempo real desde una webcam y detectar gestos de mano, mientras empleas speech_recognition y pyaudio para reconocer comandos de voz simples como 'azul', 'rotar' o 'mostrar'. Procesa simultÃ¡neamente ambas seÃ±ales de entrada utilizando hilos o bucles coordinados, e implementa lÃ³gica condicional que combine ambos tipos de entrada. Por ejemplo: cambiar el color solo si se dice 'cambiar' y la mano estÃ¡ abierta, o mover un objeto solo si se pronuncia 'mover' y se muestra un gesto de dos dedos. Finalmente, diseÃ±a una escena visual interactiva con tkinter, que responda dinÃ¡micamente a estas combinaciones de entrada.
```



---

## ğŸ’¬ ReflexiÃ³n Final

- Â¿QuÃ© aprendiste o reforzaste con este taller?
- Â¿QuÃ© parte fue mÃ¡s compleja o interesante?
- Â¿QuÃ© mejorarÃ­as o quÃ© aplicarÃ­as en futuros proyectos?

---

## âœ… Checklist de Entrega

- [X] Carpeta `2025-05-31_taller_interfaces_multimodales_voz_gestos`
- [ ] CÃ³digo limpio y funcional
- [ ] GIF incluido con nombre descriptivo
- [ ] Visualizaciones o mÃ©tricas exportadas
- [ ] README completo y claro
- [ ] Commits descriptivos en inglÃ©s

---